# fine-tune-llm-local-mbp
This repo walks through fine tuning a quantized version of the Mistral-7b-Instruct Large Language Model locally on a 16GB M1 Macbook Pro.
